{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json     #Fetching the externally sourced data\n",
    "new_data = []\n",
    "with open('news_data.json',encoding=\"utf8\") as rf:   \n",
    "    data = json.load(rf)\n",
    "for i in data:\n",
    "    new_data.append(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list = []    #Fetching the Train data set and making two list of text and labels\n",
    "train_labels = []\n",
    "count = 0\n",
    "with open(\"train.json\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "for key,value in data.items():\n",
    "    new_list.append(data[key][\"text\"])\n",
    "    train_labels.append(data[key][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = 0    # Adding the externally sourced data to the train files\n",
    "for i in range(0,1200):\n",
    "    new_list.append(new_data[i])\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list = []   # Fetching the Dev data set\n",
    "dev_labels = []\n",
    "with open(\"C:\\\\Users\\\\hp\\\\Desktop\\\\Uni\\\\Sem 3\\\\NLP\\\\Assignment 2\\\\project-files\\\\project-files\\\\dev.json\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "for key,value in data.items():\n",
    "    dev_list.append(data[key][\"text\"])\n",
    "    dev_labels.append(data[key][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk   # Preprocessing using NLTK package, to clean and filter the data sets\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")  # Using Regex tokenizer to remove special characters from the text\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "train_tweets = []\n",
    "dev_tweets = []\n",
    "for i in range(0,len(new_list)):\n",
    "    new_list[i] = new_list[i].lower()\n",
    "    prepro = tokenizer.tokenize(new_list[i])\n",
    "    preproStopWords = [w for w in prepro if not w in stopwords]\n",
    "    filtered_sentence = (\" \").join(preproStopWords)\n",
    "    train_tweets.append(filtered_sentence)\n",
    "prepro,preproStopWords,stemmed = [],[],[]\n",
    "\n",
    "for i in range(0,len(dev_list)):\n",
    "    dev_list[i] = dev_list[i].lower()\n",
    "    prepro = tokenizer.tokenize(dev_list[i])\n",
    "    preproStopWords = [w for w in prepro if not w in stopwords]\n",
    "    filtered_sentence = (\" \").join(preproStopWords)\n",
    "    dev_tweets.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=10000000,norm='l1')\n",
    "X_train = vectorizer.fit_transform(train_tweets)\n",
    "X_dev = vectorizer.transform(dev_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6716417910447762\n",
      "Recall    = 0.9\n",
      "F1        = 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree  # Implementing Decision Tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, train_labels)\n",
    "pred = clf.predict(X_dev)\n",
    "\n",
    "print(accuracy_score(pred,dev_labels))\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(dev_labels, pred, pos_label=1, average=\"binary\")\n",
    "\n",
    "print(\"Performance on the positive class (documents with misinformation):\")\n",
    "print(\"Precision =\", p)\n",
    "print(\"Recall    =\", r)\n",
    "print(\"F1        =\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk     # Again preprocessing, as data types different for CountVectorizer and HashingVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "X_train = []\n",
    "for i in range(0,len(new_list)):\n",
    "    new_list[i] = new_list[i].lower()\n",
    "    prepro = tokenizer.tokenize(new_list[i])\n",
    "    preproStopWords = [w for w in prepro if not w in stopwords]\n",
    "    #stemmed = [porter.stem(word) for word in preproStopWords]\n",
    "    filtered_sentence = (\" \").join(preproStopWords)\n",
    "    X_train.append(filtered_sentence)\n",
    "X_dev = []\n",
    "for i in range(0,len(dev_list)):\n",
    "    dev_list[i] = dev_list[i].lower()\n",
    "    prepro = tokenizer.tokenize(dev_list[i])\n",
    "    preproStopWords = [w for w in prepro if not w in stopwords]\n",
    "    #stemmed = [porter.stem(word) for word in preproStopWords]\n",
    "    filtered_sentence = (\" \").join(preproStopWords)\n",
    "    X_dev.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.7014925373134329\n",
      "Recall    = 0.94\n",
      "F1        = 0.8034188034188033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline  # Implementing LinearSVC model \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('clf', LinearSVC(random_state=0, tol=1e-5,max_iter= 5000)),\n",
    "               ])\n",
    "sgd.fit(X_train, train_labels)\n",
    "y_pred = sgd.predict(X_dev)\n",
    "\n",
    "print(accuracy_score(y_pred,dev_labels))\n",
    "\n",
    "q, w, e, _ = precision_recall_fscore_support(dev_labels, y_pred, pos_label=1, average=\"binary\")\n",
    "\n",
    "print(\"Performance on the positive class (documents with misinformation):\")\n",
    "print(\"Precision =\", q)\n",
    "print(\"Recall    =\", w)\n",
    "print(\"F1        =\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []       # Fetching the test data \n",
    "with open(\"C:\\\\Users\\\\hp\\\\Desktop\\\\Uni\\\\Sem 3\\\\NLP\\\\Assignment 2\\\\project-files\\\\project-files\\\\test-unlabelled.json\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "for key,value in data.items():\n",
    "    test_list.append(data[key][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets = []     # Cleaning the test data set\n",
    "\n",
    "for i in range(0,len(test_list)):\n",
    "    test_list[i] = test_list[i].lower()\n",
    "    prepro = tokenizer.tokenize(test_list[i])\n",
    "    preproStopWords = [w for w in prepro if not w in stopwords]\n",
    "    filtered_sentence = (\" \").join(preproStopWords)\n",
    "    test_tweets.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = sgd.predict(test_list)  # Getting predictions for test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}   # Saving the test results\n",
    "for i in range(0,len(y_test)):\n",
    "    results[\"test-\"+str(i)] = {\"label\":int(y_test[i])}\n",
    "with open('C:\\\\Users\\\\hp\\\\Desktop\\\\Uni\\\\Sem 3\\\\NLP\\\\Assignment 2\\\\project-files\\\\project-files\\\\New Text Document.json', 'w') as outfile:\n",
    "    json.dump(results,outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
